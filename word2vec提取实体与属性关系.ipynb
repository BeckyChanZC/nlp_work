{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import gensim\n",
    "from pyhanlp import *\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据\n",
    "df = pd.read_csv('./all_bk.csv')\n",
    "print('行，列:', df.shape)\n",
    "print('字段:', df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词表\n",
    "with open('./stopwords.txt') as f:\n",
    "    read = f.read()\n",
    "    stop_words = read.splitlines()\n",
    "stop_word = [' ','',r'&#',r'x0D']#补充停用词，不断调整\n",
    "stop_words.append(stop_word)\n",
    "#print(stop_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用hanlp基于信息熵原理提取短语作为新词 保存为newword.txt\n",
    "#提取短语 作用于长文本才比较有效\n",
    "#从实验结果来看，此方法比hanlp两种切词方法效果好\n",
    "txt_str = ''\n",
    "for i in range (0,df.shape[0]):\n",
    "    title = df.loc[i][3] #标题文本\n",
    "    txt_str = txt_str + title\n",
    "#print(txt_str)\n",
    "for new_word in HanLP.extractPhrase(txt_str, 100):\n",
    "    print(new_word)\n",
    "#new_word = HanLP.extractPhrase(txt_str, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文档-分词 注意分词质量很关键\n",
    "#需要不断查看分词结果，调整停用词表、调整新词词典\n",
    "docs_words = []\n",
    "for i in range (0,df.shape[0]):\n",
    "    doc_words = []\n",
    "    title = df.loc[i][3] #修改导入的字段\n",
    "    jieba.add_word('反恐法')\n",
    "    jieba.add_word('三股势力')#加入新词 ，不断调整 \n",
    "    jieba.add_word('斯里兰卡')\n",
    "    jieba.add_word('去极端化')\n",
    "    jieba.add_word('伊斯兰国')\n",
    "    jieba.load_userdict('./newwords.txt')#导入新词词典，可加入特定领域的词典\n",
    "    title = str(title)\n",
    "    a_seg_list = jieba.lcut(title,cut_all=False)#精确模式\n",
    "    for a_word in a_seg_list:\n",
    "        a_word = str(a_word)\n",
    "        a_word = a_word.strip()\n",
    "        if a_word not in stop_words:#过滤停用词\n",
    "            if a_word >= u'\\u4e00' and a_word <= u'\\u9fa5’:#只保留文字，根据情况调整\n",
    "                if len(a_word) >= 2:#去除单字，根据情况调整\n",
    "                    doc_words.append(a_word)\n",
    "    docs_words.append(doc_words)\n",
    "#print(docs_words)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#发现二元词、三元词，从实验结果看这个方法对于提升分词质量具有显著的效果\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(docs_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[docs_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs_words = [] #最终分词的结果\n",
    "for doc_words in docs_words:\n",
    "    new_docs_words.append(trigram_mod[bigram_mod[doc_words]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim import models\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(new_docs_words)\n",
    "corpus = [dictionary.doc2bow(text) for text in docs_words]\n",
    "#print(corpus) #对词进行唯一id编码，并统计在对应文档出现的次数 形成文档-词的词频矩阵，即词袋 word-bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf 对高频但不重要的词进行频数的调整\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "print(tfidf_model)\n",
    "print(corpus_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#构建300维的词向量，循环次数10次，忽略出现次数低于10的词\n",
    "w2v_model = Word2Vec(new_docs_words, size=300, iter=10, min_count=10)\n",
    "\n",
    "#保存模型\n",
    "w2v_model.wv.save_word2vec_format('./wz_w2v_model2', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方案一：利用tensor board进行全局词向量的降维、可视化、效果较好\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def visualize(model, output_path):\n",
    "    meta_file = \"w2x_metadata.tsv\"\n",
    "    placeholder = np.zeros((len(model.wv.index2word), model.vector_size))\n",
    "\n",
    "    with open(os.path.join(output_path, meta_file), 'wb') as file_metadata:\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            placeholder[i] = model[word]\n",
    "            # temporary solution for https://github.com/tensorflow/tensorflow/issues/9094\n",
    "            if word == '':\n",
    "                print(\"Emply Line, should replecaed by any thing else, or will cause a bug of tensorboard\")\n",
    "                file_metadata.write(\"{0}\".format('<Empty Line>').encode('utf-8') + b'\\n')\n",
    "            else:\n",
    "                file_metadata.write(\"{0}\".format(word).encode('utf-8') + b'\\n')\n",
    "\n",
    "    # define the model without training\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    embedding = tf.Variable(placeholder, trainable=False, name='w2x_metadata')\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(output_path, sess.graph)\n",
    "\n",
    "    # adding into projector\n",
    "    config = projector.ProjectorConfig()\n",
    "    embed = config.embeddings.add()\n",
    "    embed.tensor_name = 'w2x_metadata'\n",
    "    embed.metadata_path = meta_file\n",
    "\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    saver.save(sess, os.path.join(output_path, 'w2x_metadata.ckpt'))\n",
    "    print('Run `tensorboard --logdir={0}` to run visualize result on tensorboard'.format(output_path))\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(r'./wz_w2v_model’)#导入上面保存的模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方案二：提取实体、属性，进一步做针对性的社会网络分析sna；使用nodexl中的newman-claut-moorse方法效果佳\n",
    "#提取实体\n",
    "from jieba import posseg\n",
    "from collections import Counter\n",
    "import gensim\n",
    "## 词性标注\n",
    "new_posseg =[]\n",
    "for i in tqdm(range(0,df.shape[0])):\n",
    "    contents = str(df['标题'][i]) + str(df['内容'][i])\n",
    "    a_new_posseg = jieba.posseg.lcut(contents)#精确模式\n",
    "    new_posseg.append(a_new_posseg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for j in range(0,len(new_posseg)):\n",
    "    for a,b in new_posseg[j]:\n",
    "        a_out = []\n",
    "        if b in ['ns','nr','nrf','nsf','nt','nz']:\n",
    "            a_out.append(str(a))\n",
    "            a_out.append(str(b))\n",
    "            out.append(a_out)\n",
    "            \n",
    "cixing = pd.DataFrame(out, columns=['word', 'atr']) \n",
    "cixing = cixing.groupby(['word','atr']).size().sort_values(ascending = False)\n",
    "cixing.to_csv('./wz_实体_词性_词频.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cixing = pd.read_csv('./wz_实体_词性_词频.csv',header = None)\n",
    "n_com = []\n",
    "for i in range(0,200):\n",
    "    a_com = []\n",
    "    n = cixing.iloc[i][0]#词\n",
    "    com = cixing.iloc[i][2]#词频\n",
    "    atr = cixing.iloc[i][1]#词性\n",
    "    a_com.append(n)\n",
    "    a_com.append(com)\n",
    "    a_com.append(atr)\n",
    "    n_com.append(a_com)\n",
    "print(n_com)#查看词频最高的前200个实体，再进行筛选\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将筛选好的实体放在字典中，数据结构：{ \"name\": \"傅恒\", \"id\": \"0\", \"size\": 12.77 },\n",
    "node_res = []\n",
    "for i in range(len(n_com)):\n",
    "    node = {}\n",
    "    a = n_com[i]\n",
    "    node[\"name\"] = a[0] \n",
    "    node[\"id\"] = str(i)\n",
    "    node[\"size\"] = a[1]\n",
    "    node_res.append(node)\n",
    "print(node_res) #可用作为利用E chart做图的输入数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node = pd.DataFrame(out, columns=['word','frequency', 'atr']) \n",
    "Node.to_csv(‘./node1.csv’) \n",
    "#一级实体提取完毕\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取一级实体的关系，数据结构： { \"id\": \"1\", \"source\": \"0\", \"target\": \"7” }\n",
    "out = []\n",
    "count = 0\n",
    "for i in range(0,len(node_res)):\n",
    "    for j in range(i+1,len(node_res)):\n",
    "        a_out = []\n",
    "        name1 = node_res[i]['name']\n",
    "        name2 = node_res[j]['name']\n",
    "        sim = w2v_model.wv.similarity(name1,name2) #计算余弦相似度\n",
    "        print(name1,name2,sim)\n",
    "        if sim < 1 and sim > 0.3  : #调节阈值\n",
    "            a_out.append(node_res[i]['id'])\n",
    "            a_out.append(node_res[j]['id'])\n",
    "            a_out.append(sim)\n",
    "            out.append(a_out)\n",
    "sim_matrix = pd.DataFrame(out, columns=['source', 'target', 'sim'])   \n",
    "new_sim_matrix = sim_matrix.sort_values('sim',ascending = False).reset_index()\n",
    "new_sim_matrix.to_csv('./link1.csv’)\n",
    "#一级实体关系提取完毕\n",
    "#字典格式\n",
    "links = []\n",
    "for i in range (0,new_sim_matrix.shape[0]):\n",
    "    link = {}\n",
    "    link['id'] = str(i)\n",
    "    link['source'] = new_sim_matrix['source'][i]\n",
    "    link['target'] = new_sim_matrix['target'][i]\n",
    "    links.append(link)  \n",
    "print(links) #可用作为利用E chart做图的输入数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#二级关系 描述一级主体的属性\n",
    "out = []\n",
    "atr_id = len(node_res)\n",
    "for node in node_res:\n",
    "    atr = w2v_model.wv.most_similar(node['name'])\n",
    "    for j in range(len(atr)):\n",
    "        a_out = []\n",
    "        a_out.append(node['name'])\n",
    "        a_out.append(node['id'])\n",
    "        a_out.append(atr[j][0])\n",
    "        a_out.append(str(atr_id))\n",
    "        a_out.append(atr[j][1])\n",
    "        out.append(a_out)\n",
    "        atr_id = atr_id + 1\n",
    "        \n",
    "atribution = pd.DataFrame(out,columns = ['source','source_id’,'target','target_id','sim'])\n",
    "atribution.to_csv('./link2.csv')  \n",
    "#二级关系提取完毕  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取二级关系的属性\n",
    "data_flat = [w for w_list in new_docs_words for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "node2 = atribution\n",
    "out = []\n",
    "for i in range(0,node2.shape[0]):\n",
    "    a_out = []\n",
    "    a_out.append(node2['target_id’][i])\n",
    "    a_out.append(node2['target’][i])\n",
    "    a_out.append(counter[node2['target'][i]])\n",
    "    out.append(a_out)\n",
    "Node2 = pd.DataFrame(out,columns = ['target_id','target','frequency']) \n",
    "Node2.to_csv(‘./node2.csv’)\n",
    "#二级属性提取完毕     \n",
    "#下一步，用nodexl计算sna指标、聚类、画图\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
