{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“kashgari_bert_weibo_sentiment.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeckyChanZC/nlp_work/blob/master/%E2%80%9Ckashgari_bert_weibo_sentiment_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwy5SzLFv9-J",
        "colab_type": "text"
      },
      "source": [
        "# 安装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCBJDU_jXMiZ",
        "colab_type": "code",
        "outputId": "09dcf1f0-6c59-42b3-ea92-0e280c5a52c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 46kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.27.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.1.8)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (45.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFNGgIhGYDMJ",
        "colab_type": "code",
        "outputId": "1df90443-b0b4-4330-bec9-3a97bd2b8b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wACf-djlYRGW",
        "colab_type": "code",
        "outputId": "903d7785-9bde-4761-91e1-257c48b2f87a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install kashgari-tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kashgari-tf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/42/804e5199f4c9c2a461ff05773f61abe2094f2d430dac258a3a49a4b2a32d/kashgari_tf-0.5.5-py3-none-any.whl (75kB)\n",
            "\r\u001b[K     |████▍                           | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari-tf) (2.8.0)\n",
            "Collecting keras-gpt-2>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/df/19/d11eac066ffcb61ec9edd23c02e4651eaa31f1f67c167a636dd90b6142a4/keras-gpt-2-0.14.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari-tf) (0.22.1)\n",
            "Collecting seqeval==0.0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/55/dd/3bf1c646c310daabae47fceb84ea9ab66df7f518a31a89955290d82b8100/seqeval-0.0.10-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari-tf) (0.25.3)\n",
            "Collecting keras-bert>=0.50.0\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n",
            "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari-tf) (3.6.0)\n",
            "Collecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 201kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->kashgari-tf) (1.12.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari-tf) (2.2.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari-tf) (2019.12.20)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/54/0c/fede535ac576c03863c44bf2e0bf051fe21f5e10103631b6b6236ae446f3/keras-transformer-0.32.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari-tf) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari-tf) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari-tf) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari-tf) (2018.9)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari-tf) (1.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-gpt-2>=0.8.0->kashgari-tf) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-gpt-2>=0.8.0->kashgari-tf) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-gpt-2>=0.8.0->kashgari-tf) (1.0.8)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (1.11.15)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (2.49.0)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim>=3.5.0->kashgari-tf) (0.15.2)\n",
            "Building wheels for collected packages: keras-gpt-2, keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-gpt-2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-gpt-2: filename=keras_gpt_2-0.14.0-cp36-none-any.whl size=10525 sha256=72c2378718672873d8c1936dd3a8c89ccee9234a70bdc54de9a76dfde96345f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d8/06/ba8216a77a55b8ba4a5c3932c7df93e87eeaea83ced27822aa\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=f33d6ba79c2c97140a8d5420468d710286a5005651d22bcba8e11b8097fafe39\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.32.0-cp36-none-any.whl size=13266 sha256=3ff7808e129c72e465f6a9a3fed7291d9730af37fb102b347a1305478ada68dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f0/ce/82fa5d024d5ef8e263f26a50dcee23820efe245680ce9c922a\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=7cf925b463a18d81d739c2f18435405f34dd7af81d10d3a33f314d6fab25bde6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=61ce7dfe617bf57b6e668e8cae927dff9f33570d8f1f73111f699eb1c9917a7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=2ee127fe558c113ab0ebd50816de80a14c79d249dece4c439769064b0f9b8612\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=1a19134ec2b82b6d0bb5e20cdb075f150756bb768d8a7fb573484471bdec660d\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=3f57d56408146935a02b7330634f031e88715605a9adc1ddf4b24c7581551fbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=9637a3e6e9b5367c25930395746e0e4bcb3d872d8bee6e8c489c5c04ef58e6b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-gpt-2 keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-gpt-2, seqeval, keras-bert, kashgari-tf\n",
            "  Found existing installation: numpy 1.17.5\n",
            "    Uninstalling numpy-1.17.5:\n",
            "      Successfully uninstalled numpy-1.17.5\n",
            "Successfully installed kashgari-tf-0.5.5 keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-gpt-2-0.14.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.32.0 numpy-1.16.4 seqeval-0.0.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4vwDhnLYB4o",
        "colab_type": "code",
        "outputId": "0beb4245-30f3-4a89-f466-178fd5f994a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "from kashgari.tasks.classification import BiLSTM_Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n",
            "WARNING:root:\n",
            "╭─────────────────────────────────────────────────────────────────────────╮\n",
            "│ ◎ ○ ○ ░░░░░░░░░░░░░░░░░░░░░  Important Message  ░░░░░░░░░░░░░░░░░░░░░░░░│\n",
            "├─────────────────────────────────────────────────────────────────────────┤\n",
            "│                                                                         │\n",
            "│              We renamed again for consistency and clarity.              │\n",
            "│                   From now on, it is all `kashgari`.                    │\n",
            "│  Changelog: https://github.com/BrikerMan/Kashgari/releases/tag/v1.0.0   │\n",
            "│                                                                         │\n",
            "│         | Backend          | pypi version   | desc           |          │\n",
            "│         | ---------------- | -------------- | -------------- |          │\n",
            "│         | TensorFlow 2.x   | kashgari 2.x.x | coming soon    |          │\n",
            "│         | TensorFlow 1.14+ | kashgari 1.x.x |                |          │\n",
            "│         | Keras            | kashgari 0.x.x | legacy version |          │\n",
            "│                                                                         │\n",
            "╰─────────────────────────────────────────────────────────────────────────╯\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xut7lkvyY18X",
        "colab_type": "code",
        "outputId": "a1ea7146-4097-45fd-f31c-cfac44740a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import kashgari\n",
        "kashgari.config.use_cudnn_cell = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lynjacWpbVgq",
        "colab_type": "code",
        "outputId": "4106b4d9-3f10-4b0e-f75f-f9f2ef6ba18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
        "                                          model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGG6Kt80Y9Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BasicTokenizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THrwhuyVwIxi",
        "colab_type": "text"
      },
      "source": [
        "# 数据\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8hisNplc76M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J125L2Y0blWv",
        "colab_type": "code",
        "outputId": "2a83391e-4e36-429d-d662-f14eea5a6b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/data/weibo_sentiment/simplifyweibo_4_moods.csv')\n",
        "df.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>﻿啊呀呀！要死啦！么么么！只穿外套就好了，我认为里面那件很多余啊周小伦喜歡 你各種 五角星的...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>嗯……既然大姚通知了……那我也表示下收到……姚，你知道吗？假如外星人入侵地球，只要摧毁我们的...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>风格不一样嘛，都喜欢！最喜欢哪张？</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                             review\n",
              "0      0  ﻿啊呀呀！要死啦！么么么！只穿外套就好了，我认为里面那件很多余啊周小伦喜歡 你各種 五角星的...\n",
              "1      0  嗯……既然大姚通知了……那我也表示下收到……姚，你知道吗？假如外星人入侵地球，只要摧毁我们的...\n",
              "2      0                                  风格不一样嘛，都喜欢！最喜欢哪张？"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMoogVMAinLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPN9B7EZifio",
        "colab_type": "code",
        "outputId": "f207cedf-2111-4f37-bcd9-bb5adbca4e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(361744, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ_-Rjw2b2B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna()#一定要随机分布！！！\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=.2, random_state=2)\n",
        "train, valid = train_test_split(train, test_size=.2, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XCpzCryTEX6",
        "colab_type": "code",
        "outputId": "5ef6cc0c-dbb0-4a42-8a9b-506191f6fff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(valid.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(231516, 2)\n",
            "(72349, 2)\n",
            "(57879, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hqwCFLhTXl2",
        "colab_type": "code",
        "outputId": "22a5b172-bd71-45fb-fdaf-4679680363f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(train.head(3))\n",
        "print(test.head(3))\n",
        "print(valid.head(3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        label                                             review\n",
            "168072      0                                     特喜欢看猫的这个POSE 早\n",
            "58409       0  管饭1，亲城管，远小贩，此我朝所以兴隆也。2，城管百战死，小贩十年归。3，城管到死死不尽，小...\n",
            "28351       0  喝茶，看他被他告的人骂。NND 。无论如何不搭理我，全把我当空气了。退而求其次，做个顾问吧，...\n",
            "        label                                             review\n",
            "312534      3         oh ...受不了了！！眼泪................828無 法無 天-成都\n",
            "44730       0  母不嫌女美，父不妒 儿强。爱就是无私的滋养，爱就是完全的包容我亲历了荷的萌芽新生，青涩岁月，...\n",
            "326744      3  你们保重惊闻爸爸同事的亲戚在大火中不幸罹难，那位阿姨从房里逃出来时给丈夫打电话说，我逃出来了...\n",
            "        label                                             review\n",
            "82426       0  严肃点，没时间跟你开玩笑--有本事来惹爷试试！喔!太历害啦......猫猫的动作充滿 挑釁 ...\n",
            "30249       0                                       : 還 是絲 襪 ，，，\n",
            "289928      2      回复我要吃你那火锅~ 5555555！露香肩偶尔咱也得来把小性感~ ~ > _ < !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2x6aBRhShZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 准备训练测试数据集\n",
        "train_x = list(train['review'].apply(lambda x: tokenizer.tokenize(x)))\n",
        "train_y = list(train['label'])\n",
        "\n",
        "test_x = list(test['review'].apply(lambda x: tokenizer.tokenize(x)))\n",
        "test_y = list(test['label'])\n",
        "\n",
        "valid_x = list(valid['review'].apply(lambda x: tokenizer.tokenize(x)))\n",
        "valid_y = list(valid['label'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWy42pq8UXRJ",
        "colab_type": "code",
        "outputId": "1839e055-0cd5-4a36-f2e3-a829636b749a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(train_x[0])\n",
        "print(test_x[0])\n",
        "print(valid_x[0])\n",
        "print(train_y[0])\n",
        "print(test_y[0])\n",
        "print(valid_y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['特', '喜', '欢', '看', '猫', '的', '这', '个', 'pose', '早']\n",
            "['oh', '.', '.', '.', '受', '不', '了', '了', '！', '！', '眼', '泪', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '828', '無', '法', '無', '天', '-', '成', '都']\n",
            "['严', '肃', '点', '，', '没', '时', '间', '跟', '你', '开', '玩', '笑', '-', '-', '有', '本', '事', '来', '惹', '爷', '试', '试', '！', '喔', '!', '太', '历', '害', '啦', '.', '.', '.', '.', '.', '.', '猫', '猫', '的', '动', '作', '充', '滿', '挑', '釁', '@', '^', '^', '@']\n",
            "0\n",
            "3\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MIxh6-lwYVu",
        "colab_type": "text"
      },
      "source": [
        "# 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCZZvgpdcM3T",
        "colab_type": "code",
        "outputId": "c3a3f945-56a8-4bd8-8c8f-0452f23bb73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BERT_PATH = '/content/drive/My Drive/vocab_file'\n",
        "\n",
        "# 初始化 Embedding\n",
        "embed = BERTEmbedding(BERT_PATH,\n",
        "                     task=kashgari.CLASSIFICATION,\n",
        "                     sequence_length=30)\n",
        "\n",
        "# 使用 embedding 初始化模型\n",
        "model = BiLSTM_Model(embed)\n",
        "# 先只训练一轮\n",
        "#model.fit(train_x, train_y, valid_x, valid_y, batch_size=1024, epochs=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:seq_len: 30\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuOtwF0v8Lcj",
        "colab_type": "code",
        "outputId": "5cab324d-3a2f-4b1c-c881-ee5d2a15b5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.python import keras\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='/content/drive/My Drive/data/weibo_sentiment/logs1', update_freq=1000)\n",
        "\n",
        "# 这是 Kashgari 内置回调函数，会在训练过程计算精确度，召回率和 F1\n",
        "eval_callback = EvalCallBack(kash_model=model,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=1)\n",
        "\n",
        "model.fit(train_x,\n",
        "          train_y,\n",
        "          valid_x,\n",
        "          valid_y,\n",
        "          batch_size=4000,\n",
        "          epochs=50,\n",
        "          callbacks=[eval_callback, tf_board_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 30, 768), (2 16226304    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 30, 768)      1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 30, 768)      0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 30, 768)      23040       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 30, 768)      0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 30, 768)      1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 30, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 30, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 30, 768)      0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 30, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 30, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 30, 768)      0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 30, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 30, 768)      0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 30, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 30, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 30, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 30, 768)      0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 30, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 30, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Output (Concatenate)    (None, 30, 3072)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "non_masking_layer (NonMaskingLa (None, 30, 3072)     0           Encoder-Output[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 256)          3278848     non_masking_layer[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4)            1028        bidirectional[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 104,586,756\n",
            "Trainable params: 3,279,876\n",
            "Non-trainable params: 101,306,880\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 1.1772 - acc: 0.5525\n",
            "epoch: 0 precision: 0.489192, recall: 0.571848, f1: 0.484033\n",
            "58/58 [==============================] - 365s 6s/step - loss: 1.1752 - acc: 0.5529 - val_loss: 1.0727 - val_acc: 0.5718\n",
            "Epoch 2/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 1.0592 - acc: 0.5796\n",
            "epoch: 1 precision: 0.503453, recall: 0.577999, f1: 0.502997\n",
            "58/58 [==============================] - 353s 6s/step - loss: 1.0592 - acc: 0.5795 - val_loss: 1.0585 - val_acc: 0.5748\n",
            "Epoch 3/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 1.0411 - acc: 0.5861\n",
            "epoch: 2 precision: 0.505577, recall: 0.581143, f1: 0.508277\n",
            "58/58 [==============================] - 353s 6s/step - loss: 1.0410 - acc: 0.5862 - val_loss: 1.0418 - val_acc: 0.5782\n",
            "Epoch 4/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 1.0286 - acc: 0.5909\n",
            "epoch: 3 precision: 0.508456, recall: 0.581955, f1: 0.518356\n",
            "58/58 [==============================] - 353s 6s/step - loss: 1.0291 - acc: 0.5907 - val_loss: 1.0385 - val_acc: 0.5796\n",
            "Epoch 5/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 1.0192 - acc: 0.5938\n",
            "epoch: 4 precision: 0.505930, recall: 0.582111, f1: 0.516676\n",
            "58/58 [==============================] - 352s 6s/step - loss: 1.0190 - acc: 0.5938 - val_loss: 1.0340 - val_acc: 0.5815\n",
            "Epoch 6/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 1.0096 - acc: 0.5971\n",
            "epoch: 5 precision: 0.512421, recall: 0.583614, f1: 0.518732\n",
            "58/58 [==============================] - 353s 6s/step - loss: 1.0096 - acc: 0.5971 - val_loss: 1.0310 - val_acc: 0.5833\n",
            "Epoch 7/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9996 - acc: 0.6005\n",
            "epoch: 6 precision: 0.512241, recall: 0.585152, f1: 0.521507\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.9995 - acc: 0.6006 - val_loss: 1.0244 - val_acc: 0.5848\n",
            "Epoch 8/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9895 - acc: 0.6036\n",
            "epoch: 7 precision: 0.507817, recall: 0.584841, f1: 0.518630\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.9899 - acc: 0.6034 - val_loss: 1.0248 - val_acc: 0.5859\n",
            "Epoch 9/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9794 - acc: 0.6072\n",
            "epoch: 8 precision: 0.508206, recall: 0.583510, f1: 0.521152\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.9795 - acc: 0.6073 - val_loss: 1.0194 - val_acc: 0.5850\n",
            "Epoch 10/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9669 - acc: 0.6119\n",
            "epoch: 9 precision: 0.510459, recall: 0.578362, f1: 0.524756\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.9670 - acc: 0.6119 - val_loss: 1.0244 - val_acc: 0.5764\n",
            "Epoch 11/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9538 - acc: 0.6158\n",
            "epoch: 10 precision: 0.509832, recall: 0.579105, f1: 0.526490\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.9541 - acc: 0.6157 - val_loss: 1.0215 - val_acc: 0.5815\n",
            "Epoch 12/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9406 - acc: 0.6201\n",
            "epoch: 11 precision: 0.506786, recall: 0.583839, f1: 0.520002\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.9402 - acc: 0.6202 - val_loss: 1.0261 - val_acc: 0.5834\n",
            "Epoch 13/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9276 - acc: 0.6241\n",
            "epoch: 12 precision: 0.511429, recall: 0.577463, f1: 0.527224\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.9276 - acc: 0.6241 - val_loss: 1.0249 - val_acc: 0.5778\n",
            "Epoch 14/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9148 - acc: 0.6284\n",
            "epoch: 13 precision: 0.513501, recall: 0.569550, f1: 0.529367\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.9150 - acc: 0.6282 - val_loss: 1.0291 - val_acc: 0.5708\n",
            "Epoch 15/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.9000 - acc: 0.6341\n",
            "epoch: 14 precision: 0.505999, recall: 0.578621, f1: 0.523982\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.9000 - acc: 0.6340 - val_loss: 1.0244 - val_acc: 0.5789\n",
            "Epoch 16/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.8853 - acc: 0.6370\n",
            "epoch: 15 precision: 0.513935, recall: 0.571088, f1: 0.527993\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.8853 - acc: 0.6370 - val_loss: 1.0307 - val_acc: 0.5711\n",
            "Epoch 17/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.8691 - acc: 0.6438\n",
            "epoch: 16 precision: 0.509910, recall: 0.571814, f1: 0.527063\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.8692 - acc: 0.6437 - val_loss: 1.0275 - val_acc: 0.5726\n",
            "Epoch 18/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.8550 - acc: 0.6480\n",
            "epoch: 17 precision: 0.516088, recall: 0.566060, f1: 0.532666\n",
            "58/58 [==============================] - 354s 6s/step - loss: 0.8551 - acc: 0.6479 - val_loss: 1.0376 - val_acc: 0.5658\n",
            "Epoch 19/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.8412 - acc: 0.6525\n",
            "epoch: 18 precision: 0.515202, recall: 0.564799, f1: 0.532958\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.8416 - acc: 0.6524 - val_loss: 1.0388 - val_acc: 0.5644\n",
            "Epoch 20/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.8301 - acc: 0.6573\n",
            "epoch: 19 precision: 0.518522, recall: 0.557162, f1: 0.533490\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.8303 - acc: 0.6572 - val_loss: 1.0472 - val_acc: 0.5571\n",
            "Epoch 21/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.8124 - acc: 0.6625\n",
            "epoch: 20 precision: 0.524910, recall: 0.551996, f1: 0.535252\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.8128 - acc: 0.6623 - val_loss: 1.0642 - val_acc: 0.5504\n",
            "Epoch 22/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7996 - acc: 0.6674\n",
            "epoch: 21 precision: 0.515535, recall: 0.570328, f1: 0.532346\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.7996 - acc: 0.6674 - val_loss: 1.0414 - val_acc: 0.5715\n",
            "Epoch 23/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7867 - acc: 0.6709\n",
            "epoch: 22 precision: 0.512613, recall: 0.565490, f1: 0.532001\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.7870 - acc: 0.6708 - val_loss: 1.0507 - val_acc: 0.5646\n",
            "Epoch 24/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7729 - acc: 0.6766\n",
            "epoch: 23 precision: 0.516812, recall: 0.561292, f1: 0.532825\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.7736 - acc: 0.6763 - val_loss: 1.0575 - val_acc: 0.5630\n",
            "Epoch 25/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7632 - acc: 0.6800\n",
            "epoch: 24 precision: 0.525523, recall: 0.553638, f1: 0.536476\n",
            "58/58 [==============================] - 354s 6s/step - loss: 0.7634 - acc: 0.6799 - val_loss: 1.0755 - val_acc: 0.5513\n",
            "Epoch 26/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7480 - acc: 0.6852\n",
            "epoch: 25 precision: 0.516190, recall: 0.558838, f1: 0.533158\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.7483 - acc: 0.6850 - val_loss: 1.0635 - val_acc: 0.5593\n",
            "Epoch 27/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7387 - acc: 0.6886\n",
            "epoch: 26 precision: 0.529492, recall: 0.550873, f1: 0.537150\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.7393 - acc: 0.6884 - val_loss: 1.0825 - val_acc: 0.5512\n",
            "Epoch 28/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7271 - acc: 0.6918\n",
            "epoch: 27 precision: 0.522659, recall: 0.562778, f1: 0.536794\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.7274 - acc: 0.6916 - val_loss: 1.0742 - val_acc: 0.5616\n",
            "Epoch 29/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7179 - acc: 0.6950\n",
            "epoch: 28 precision: 0.521472, recall: 0.565058, f1: 0.537361\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.7188 - acc: 0.6946 - val_loss: 1.0771 - val_acc: 0.5672\n",
            "Epoch 30/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.7052 - acc: 0.7004\n",
            "epoch: 29 precision: 0.525203, recall: 0.561447, f1: 0.538993\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.7056 - acc: 0.7001 - val_loss: 1.0821 - val_acc: 0.5613\n",
            "Epoch 31/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6924 - acc: 0.7043\n",
            "epoch: 30 precision: 0.520703, recall: 0.567321, f1: 0.538060\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.6926 - acc: 0.7042 - val_loss: 1.0877 - val_acc: 0.5673\n",
            "Epoch 32/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6872 - acc: 0.7056\n",
            "epoch: 31 precision: 0.528333, recall: 0.553033, f1: 0.535380\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.6871 - acc: 0.7057 - val_loss: 1.1027 - val_acc: 0.5554\n",
            "Epoch 33/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6766 - acc: 0.7098Epoch 34/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6674 - acc: 0.7127\n",
            "epoch: 33 precision: 0.527138, recall: 0.555953, f1: 0.539658\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.6675 - acc: 0.7127 - val_loss: 1.1040 - val_acc: 0.5567\n",
            "Epoch 35/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6594 - acc: 0.7172\n",
            "epoch: 34 precision: 0.523816, recall: 0.561326, f1: 0.538780\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.6597 - acc: 0.7170 - val_loss: 1.1034 - val_acc: 0.5619\n",
            "Epoch 36/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6510 - acc: 0.7188\n",
            "epoch: 35 precision: 0.526905, recall: 0.558251, f1: 0.540167\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.6513 - acc: 0.7187 - val_loss: 1.1134 - val_acc: 0.5587\n",
            "Epoch 37/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6408 - acc: 0.7224\n",
            "epoch: 36 precision: 0.528519, recall: 0.559858, f1: 0.539743\n",
            "58/58 [==============================] - 353s 6s/step - loss: 0.6410 - acc: 0.7224 - val_loss: 1.1250 - val_acc: 0.5590\n",
            "Epoch 38/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6387 - acc: 0.7239\n",
            "epoch: 37 precision: 0.534616, recall: 0.550856, f1: 0.538503\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.6390 - acc: 0.7237 - val_loss: 1.1439 - val_acc: 0.5504\n",
            "Epoch 39/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6281 - acc: 0.7268\n",
            "epoch: 38 precision: 0.533230, recall: 0.538347, f1: 0.535588\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.6285 - acc: 0.7267 - val_loss: 1.1450 - val_acc: 0.5387\n",
            "Epoch 40/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6203 - acc: 0.7295\n",
            "epoch: 39 precision: 0.529487, recall: 0.552739, f1: 0.538955\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.6206 - acc: 0.7294 - val_loss: 1.1444 - val_acc: 0.5535\n",
            "Epoch 41/50\n",
            "57/58 [============================>.] - ETA: 4s - loss: 0.6109 - acc: 0.7340\n",
            "epoch: 40 precision: 0.530319, recall: 0.552066, f1: 0.539060\n",
            "58/58 [==============================] - 352s 6s/step - loss: 0.6112 - acc: 0.7337 - val_loss: 1.1567 - val_acc: 0.5520\n",
            "Epoch 42/50\n",
            "48/58 [=======================>......] - ETA: 41s - loss: 0.6036 - acc: 0.7353"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-79b59a693870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           callbacks=[eval_callback, tf_board_callback])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kashgari/tasks/base_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_validate, y_validate, batch_size, epochs, callbacks, fit_kwargs, shuffle)\u001b[0m\n\u001b[1;32m    308\u001b[0m                                                \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                                                \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                                                **fit_kwargs)\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     def fit_without_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myzUHFB8pe_j",
        "colab_type": "code",
        "outputId": "d0b1979e-4532-452e-d375-e29f92e3559b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model.evaluate(test_x, test_y, batch_size=4000)\n",
        "#看来类别太多影响效果，要划分二级才行，下一步要直接删减类别"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7373    0.8512    0.7902     39807\n",
            "           1     0.4059    0.2620    0.3185     10380\n",
            "           2     0.2133    0.1881    0.1999     11084\n",
            "           3     0.2257    0.2021    0.2133     11078\n",
            "\n",
            "    accuracy                         0.5657     72349\n",
            "   macro avg     0.3956    0.3759    0.3805     72349\n",
            "weighted avg     0.5312    0.5657    0.5437     72349\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p50KAHVtphGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/data/weibo_sentiment/weibo_sentiment_bert_model_0.56')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfZ9mDELwofg",
        "colab_type": "text"
      },
      "source": [
        "# 预测"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsNSV8nNR1Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kashgari.utils import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG7Y_RpWROCm",
        "colab_type": "code",
        "outputId": "b560d670-b440-4f25-9ba4-afebfe34eb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = load_model('/content/drive/My Drive/data/weibo_sentiment/weibo_sentiment_bert_model_0.56')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV-Vc98FSzpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27V6w8HOHIkv",
        "colab_type": "code",
        "outputId": "51575b88-8a86-4fef-fdb0-e9748bd369a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "user = pd.read_csv('/content/drive/My Drive/data/weibo_sentiment/es_dump_1580964980831196_7469.tsv',sep='\\t')\n",
        "user.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weibo_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>nickname</th>\n",
              "      <th>weibo_content</th>\n",
              "      <th>zhuan</th>\n",
              "      <th>zan</th>\n",
              "      <th>ping</th>\n",
              "      <th>r_weibo_id</th>\n",
              "      <th>r_user_id</th>\n",
              "      <th>r_nickname</th>\n",
              "      <th>r_weibo_content</th>\n",
              "      <th>r_zhuan</th>\n",
              "      <th>r_zan</th>\n",
              "      <th>r_ping</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>r_time</th>\n",
              "      <th>crawler_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4467141048998003</td>\n",
              "      <td>2772119733</td>\n",
              "      <td>行云ouo</td>\n",
              "      <td>转发微博</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4466974475957154</td>\n",
              "      <td>2673484933</td>\n",
              "      <td>TIANSHIHannnn</td>\n",
              "      <td>真实案例：我朋友的外公，都死了两年了火化了，无意中家人接触到了双黄连，抱着试试看的态度，买了...</td>\n",
              "      <td>2639</td>\n",
              "      <td>10094</td>\n",
              "      <td>817</td>\n",
              "      <td>2020-02-01T04:54:43</td>\n",
              "      <td>2020-01-31T17:52:50</td>\n",
              "      <td>2020-02-01T18:44:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4466949674878961</td>\n",
              "      <td>6991432190</td>\n",
              "      <td>榴莲味的白团子</td>\n",
              "      <td>我妈昨天还和我说呢哎</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4466941530321375</td>\n",
              "      <td>5402666134</td>\n",
              "      <td>六层楼先生</td>\n",
              "      <td>别问我双黄连的事儿了，我完全不敢#老六瞎聊# …… ​​​</td>\n",
              "      <td>952</td>\n",
              "      <td>15826</td>\n",
              "      <td>1316</td>\n",
              "      <td>2020-01-31T16:14:17</td>\n",
              "      <td>2020-01-31T15:41:54</td>\n",
              "      <td>2020-02-01T20:34:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4467276504353965</td>\n",
              "      <td>2653617361</td>\n",
              "      <td>迷途ooo</td>\n",
              "      <td>转发微博</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4467271457968529</td>\n",
              "      <td>1624137625</td>\n",
              "      <td>短线刀客888</td>\n",
              "      <td>#双黄连对新型冠状病毒不具针对性##湖北红十字会将对责任人追责# \\n这么龌龊，这么阴损，这...</td>\n",
              "      <td>37</td>\n",
              "      <td>103</td>\n",
              "      <td>27</td>\n",
              "      <td>2020-02-01T13:52:59</td>\n",
              "      <td>2020-02-01T13:32:55</td>\n",
              "      <td>2020-02-01T16:10:53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           weibo_id     user_id  ...               r_time         crawler_time\n",
              "0  4467141048998003  2772119733  ...  2020-01-31T17:52:50  2020-02-01T18:44:04\n",
              "1  4466949674878961  6991432190  ...  2020-01-31T15:41:54  2020-02-01T20:34:05\n",
              "2  4467276504353965  2653617361  ...  2020-02-01T13:32:55  2020-02-01T16:10:53\n",
              "\n",
              "[3 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDrHokQmLGlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#user.columns = ['用户名','a','b','c','d','e','f']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyXoJsDOts90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user['r_weibo_content'] = user['r_weibo_content'].apply(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKS1IVsQHR08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_task = list(user['r_weibo_content'].apply(lambda x: tokenizer.tokenize(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxJj1tFPKT5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_task[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgG6NgM6CV3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.predict(user_task)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B8p3vUaTP9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYsol70ZLGf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in result:\n",
        "  print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coqVRlfJH4kK",
        "colab_type": "text"
      },
      "source": [
        "# CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4e9XzXhCbEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kashgari.tasks.classification import CNNModel\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkUlTd_kCdbk",
        "colab_type": "code",
        "outputId": "398b9ba1-346e-4df0-cd5f-d84c4fd52884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "model_cnn = CNNModel()\n",
        "model_cnn.fit(train_x, train_y, valid_x, valid_y, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 163)]             0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, 163, 100)          1477000   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 159, 128)          64128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 1,549,644\n",
            "Trainable params: 1,549,644\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1809/1809 [==============================] - 29s 16ms/step - loss: 1.0284 - acc: 0.5891 - val_loss: 0.9832 - val_acc: 0.6046\n",
            "Epoch 2/5\n",
            "1809/1809 [==============================] - 23s 12ms/step - loss: 0.9011 - acc: 0.6305 - val_loss: 0.9446 - val_acc: 0.6130\n",
            "Epoch 3/5\n",
            "1809/1809 [==============================] - 22s 12ms/step - loss: 0.7659 - acc: 0.6755 - val_loss: 0.9367 - val_acc: 0.6212\n",
            "Epoch 4/5\n",
            "1809/1809 [==============================] - 23s 13ms/step - loss: 0.6429 - acc: 0.7156 - val_loss: 0.9727 - val_acc: 0.6232\n",
            "Epoch 5/5\n",
            "1809/1809 [==============================] - 22s 12ms/step - loss: 0.5493 - acc: 0.7459 - val_loss: 1.0548 - val_acc: 0.6185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f94f1410ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KJq8zewCipj",
        "colab_type": "code",
        "outputId": "0806b733-5017-4bc9-a804-e89082da6ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model_cnn.evaluate(test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7733    0.8450    0.8075     39807\n",
            "           1     0.4823    0.3013    0.3709     10380\n",
            "           2     0.3485    0.5983    0.4404     11084\n",
            "           3     0.3314    0.0997    0.1533     11078\n",
            "\n",
            "    accuracy                         0.6151     72349\n",
            "   macro avg     0.4839    0.4611    0.4430     72349\n",
            "weighted avg     0.5988    0.6151    0.5885     72349\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHyVvS7gGhmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cnn.save('/content/drive/My Drive/data/weibo_sentiment/weibo_sentiment_cnn_model_0.61')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTW2SocgG_by",
        "colab_type": "code",
        "outputId": "57986a4a-07f1-4e9a-ba39-1393de96c581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "from kashgari.tasks.classification import BLSTMModel\n",
        "model_blst2 = BLSTMModel()\n",
        "model_blst2.fit(train_x, train_y, valid_x, valid_y, batch_size=128,epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 163)]             0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, 163, 100)          1477000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               235520    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 1,713,548\n",
            "Trainable params: 1,713,548\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1809/1809 [==============================] - 72s 40ms/step - loss: 1.0616 - acc: 0.5788 - val_loss: 1.0450 - val_acc: 0.5836\n",
            "Epoch 2/10\n",
            "1809/1809 [==============================] - 71s 39ms/step - loss: 1.0009 - acc: 0.5997 - val_loss: 1.0126 - val_acc: 0.5962\n",
            "Epoch 3/10\n",
            "1809/1809 [==============================] - 72s 40ms/step - loss: 0.9656 - acc: 0.6117 - val_loss: 0.9982 - val_acc: 0.6003\n",
            "Epoch 4/10\n",
            "1809/1809 [==============================] - 72s 40ms/step - loss: 0.9297 - acc: 0.6231 - val_loss: 0.9955 - val_acc: 0.5988\n",
            "Epoch 5/10\n",
            "1809/1809 [==============================] - 72s 40ms/step - loss: 0.8929 - acc: 0.6343 - val_loss: 1.0022 - val_acc: 0.5994\n",
            "Epoch 6/10\n",
            "1809/1809 [==============================] - 71s 39ms/step - loss: 0.8546 - acc: 0.6465 - val_loss: 1.0139 - val_acc: 0.5987\n",
            "Epoch 7/10\n",
            "1809/1809 [==============================] - 71s 39ms/step - loss: 0.8170 - acc: 0.6586 - val_loss: 1.0191 - val_acc: 0.5911\n",
            "Epoch 8/10\n",
            "1809/1809 [==============================] - 71s 39ms/step - loss: 0.7828 - acc: 0.6679 - val_loss: 1.0552 - val_acc: 0.5941\n",
            "Epoch 9/10\n",
            "1809/1809 [==============================] - 71s 39ms/step - loss: 0.7481 - acc: 0.6796 - val_loss: 1.0737 - val_acc: 0.5899\n",
            "Epoch 10/10\n",
            "1809/1809 [==============================] - 71s 39ms/step - loss: 0.7154 - acc: 0.6900 - val_loss: 1.0999 - val_acc: 0.5943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f94eeadfef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmHZZs2xOGec",
        "colab_type": "code",
        "outputId": "a0b4cac3-beb3-45fb-a34f-ca8995436dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model_blst2.evaluate(test_x, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7050    0.8695    0.7787     39807\n",
            "           1     0.4121    0.2893    0.3400     10380\n",
            "           2     0.3151    0.1936    0.2398     11084\n",
            "           3     0.3429    0.2834    0.3103     11078\n",
            "\n",
            "    accuracy                         0.5930     72349\n",
            "   macro avg     0.4438    0.4090    0.4172     72349\n",
            "weighted avg     0.5478    0.5930    0.5615     72349\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwQi3hEILlK",
        "colab_type": "text"
      },
      "source": [
        "# BiGRU Model&BERTE mbedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wNyKMU2Henl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kashgari\n",
        "from kashgari.tasks.classification import BiGRU_Model\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "\n",
        "model2 = BiGRU_Model(embed)\n",
        "\n",
        "\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='/content/drive/My Drive/data/user/logs_bi', update_freq=1000)\n",
        "\n",
        "# 这是 Kashgari 内置回调函数，会在训练过程计算精确度，召回率和 F1\n",
        "eval_callback = EvalCallBack(kash_model=model2,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=2)\n",
        "\n",
        "model2.fit(train_x,\n",
        "          train_y,\n",
        "          valid_x,\n",
        "          valid_y,\n",
        "          batch_size=100,\n",
        "          epochs=20,\n",
        "          callbacks=[eval_callback, tf_board_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbiOIYAGgJys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.evaluate(test_x, test_y, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj-M224mtTsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYzhcxjxuMDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.predict([['人','民','日','报'],['艺','人','代','表','作']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8DB5oab658b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python import keras\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='/content/drive/My Drive/data/user/logs_new_4', update_freq=1000)\n",
        "\n",
        "# 这是 Kashgari 内置回调函数，会在训练过程计算精确度，召回率和 F1\n",
        "eval_callback = EvalCallBack(kash_model=model,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=1)\n",
        "\n",
        "model.fit(train_x,\n",
        "          train_y,\n",
        "          valid_x,\n",
        "          valid_y,\n",
        "          batch_size=124,\n",
        "          epochs=4,\n",
        "          callbacks=[eval_callback, tf_board_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rARjt1jUJa1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python import keras\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='/content/drive/My Drive/data/user/logs_new_4', update_freq=1000)\n",
        "\n",
        "# 这是 Kashgari 内置回调函数，会在训练过程计算精确度，召回率和 F1\n",
        "eval_callback = EvalCallBack(kash_model=model,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=1)\n",
        "\n",
        "model.fit(train_x,\n",
        "          train_y,\n",
        "          valid_x,\n",
        "          valid_y,\n",
        "          batch_size=124,\n",
        "          epochs=1,\n",
        "          callbacks=[eval_callback, tf_board_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVOaLcG9e9GT",
        "colab_type": "text"
      },
      "source": [
        "# 其他模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQbgw8iPfu-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install textblob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrCbiBGte8L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textblob\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbxg70oIhmjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 构建符合训练格式的文本数据集合 labeled_fsets \n",
        "\n",
        "labeled_fsets = [(text, category) \n",
        "                  for category in df['label'][:2000] \n",
        "                  for text in df['review'][:2000]]\n",
        "#labeled_fsets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ-vplxvg64b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clx2Sbr4lNeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1HW-6Cft3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 划分训练集和测试集\n",
        "random.shuffle(labeled_fsets)\n",
        "train_feats =labeled_fsets[:1000]\n",
        "test_feats=labeled_fsets[1000:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDfD2mSxlj-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV02KTPsltWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 训练分类模型并利用测试集计算分类准确率\n",
        "\n",
        "cl = NaiveBayesClassifier(train_feats)\n",
        "cl.accuracy(test_feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePzmFHm2BnbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}