{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import gensim\n",
    "from pyhanlp import *\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据\n",
    "df = pd.read_csv('./all_bk.csv')\n",
    "print('行，列:', df.shape)\n",
    "print('字段:', df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词表\n",
    "with open('./stopwords.txt') as f:\n",
    "    read = f.read()\n",
    "    stop_words = read.splitlines()\n",
    "stop_word = [' ','',r'&#',r'x0D']#补充停用词，不断调整\n",
    "stop_words.append(stop_word)\n",
    "#print(stop_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用hanlp基于信息熵原理提取短语作为新词 保存为newword.txt\n",
    "#提取短语 作用于长文本才比较有效\n",
    "#从实验结果来看，此方法比hanlp两种切词方法效果好\n",
    "txt_str = ''\n",
    "for i in range (0,df.shape[0]):\n",
    "    title = df.loc[i][3] #标题文本\n",
    "    txt_str = txt_str + title\n",
    "#print(txt_str)\n",
    "for new_word in HanLP.extractPhrase(txt_str, 100):\n",
    "    print(new_word)\n",
    "#new_word = HanLP.extractPhrase(txt_str, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文档-分词 注意分词质量很关键\n",
    "#需要不断查看分词结果，调整停用词表、调整新词词典\n",
    "docs_words = []\n",
    "for i in range (0,df.shape[0]):\n",
    "    doc_words = []\n",
    "    title = df.loc[i][3] #修改导入的字段\n",
    "    jieba.add_word('反恐法')\n",
    "    jieba.add_word('三股势力')#加入新词 ，不断调整 \n",
    "    jieba.add_word('斯里兰卡')\n",
    "    jieba.add_word('去极端化')\n",
    "    jieba.add_word('伊斯兰国')\n",
    "    jieba.load_userdict('./newwords.txt')#导入新词词典，可加入特定领域的词典\n",
    "    title = str(title)\n",
    "    a_seg_list = jieba.lcut(title,cut_all=False)#精确模式\n",
    "    for a_word in a_seg_list:\n",
    "        a_word = str(a_word)\n",
    "        a_word = a_word.strip()\n",
    "        if a_word not in stop_words:#过滤停用词\n",
    "            if a_word >= u'\\u4e00' and a_word <= u'\\u9fa5’:#只保留文字，根据情况调整\n",
    "                if len(a_word) >= 2:#去除单字，根据情况调整\n",
    "                    doc_words.append(a_word)\n",
    "    docs_words.append(doc_words)\n",
    "#print(docs_words)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#发现二元词、三元词，从实验结果看这个方法对于提升分词质量具有显著的效果\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(docs_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[docs_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs_words = [] #最终分词的结果\n",
    "for doc_words in docs_words:\n",
    "    new_docs_words.append(trigram_mod[bigram_mod[doc_words]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim import models\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(new_docs_words)\n",
    "corpus = [dictionary.doc2bow(text) for text in docs_words]\n",
    "#print(corpus) #对词进行唯一id编码，并统计在对应文档出现的次数 形成文档-词的词频矩阵，即词袋 word-bag\n",
    "#tf-idf 对高频但不重要的词进行频数的调整\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "print(tfidf_model)\n",
    "print(corpus_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判断最优主题数\n",
    "def compute_coherence_values(id2word, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        #这里可以使用其他模型\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "mallet_path = './mallet-2.0.8/bin/mallet' # update this path\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(id2word=dictionary, corpus=corpus, texts=new_docs_words, start=2, limit=40, step=2)\n",
    "# Show graph\n",
    "limit=20; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方案一\n",
    "lda_model = models.LdaModel(corpus = corpus_tfidf, id2word=dictionary, num_topics=20,dtype=np.float64,\n",
    "                            iterations=6000,chunksize = 2000, passes = 1)\n",
    "#请查看各个参数的含义 #根据最优主题数调整num_topics\n",
    "lda_model.show_topics(num_topics=20, num_words=10, log=False, formatted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#方案二\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=32, id2word=dictionary)\n",
    "#根据最优主题数调整num_topics\n",
    "ldamallet.save('./mallet_LDAmodle’)\n",
    "#保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化方案一：适用于建模方案一\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis_data  = pyLDAvis.gensim.prepare(model2, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "#保存可视化结果\n",
    "pyLDAvis.save_html(vis_data,’./lda_vis.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给文档打标签\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "#在这里修改参数\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=new_docs_words)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n",
    "\n",
    "df_dominant_topic.to_csv('./mallet_wz_topic_docs.csv’)#保存结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各主题占比可视化\n",
    "topics_size = df_dominant_topic['Document_No'].groupby(df_dominant_topic['Dominant_Topic']).count()\n",
    "topics_size = topics_size.sort_values()\n",
    "topics_per = topics_size/df_dominant_topic.shape[0]\n",
    "#调节图大小\n",
    "plt.figure(figsize=(15, 10))\n",
    "# 字体会按照列表依次从电脑中查找,直到找到\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS','Microsoft YaHei','SimHei','sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False  #解决黑体符号乱码\n",
    "plt.yticks(fontsize=13)#调节刻度字体大小\n",
    "#设置文字注释\n",
    "plt.title('主题占比',fontsize=16,fontweight='bold')\n",
    "plt.ylabel('主题ID',fontsize=16)\n",
    "plt.xlabel('比例',fontsize=16)\n",
    "topics_per.plot.barh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给主题下属文章数可视化\n",
    "#调节图大小\n",
    "plt.figure(figsize=(15, 10))\n",
    "# 字体会按照列表依次从电脑中查找,直到找到\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS','Microsoft YaHei','SimHei','sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False  #解决黑体符号乱码\n",
    "plt.yticks(fontsize=13)#调节刻度字体大小\n",
    "#设置文字注释\n",
    "plt.title('主题下属文章数',fontsize=16,fontweight='bold')\n",
    "plt.ylabel('主题ID',fontsize=16)\n",
    "plt.xlabel('文章数(篇)',fontsize=16)\n",
    "x = np.array(topics_size)\n",
    "y = np.arange(32)+1\n",
    "for a,b in zip(x,y):\n",
    "    plt.text(a+10, b-1.5, '%.0f' % a, ha='center', va= 'bottom',fontsize=13)\n",
    "\n",
    "topics_size.plot.barh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#找出主题的代表文章\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(32)\n",
    "#保存结果\n",
    "sent_topics_sorteddf_mallet.to_csv('./mallet_wz_pretopic_docs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "# more colors: 'mcolors.XKCD_COLORS' mcolors.CSS4_COLORS mcolors.TABLEAU_COLORS\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0,\n",
    "                  font_path='./SourceHanSansCN-Regular.otf')\n",
    "\n",
    "topics = ldamallet.show_topics(num_topics=32,formatted=False)#调整参数\n",
    "\n",
    "fig, axes = plt.subplots(16, 2, figsize=(30,100), sharex=True, sharey=True)#调整参数\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=30))\n",
    "    plt.gca().axis('off')\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "topics = ldamallet.show_topics(num_topics=32,formatted=False)\n",
    "data_flat = [w for w_list in new_docs_words for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "topics_words = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "#保存主题-关键词数据\n",
    "df2.to_csv('./wz_topic_kw.csv’)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(16,2, figsize=(16,60), sharey=True, dpi=160) # 调节参数\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()] #颜色数量问题\n",
    "# 字体会按照列表依次从电脑中查找,直到找到\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS','Microsoft YaHei','SimHei','sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False  #解决黑体符号乱码\n",
    "#plt.rcParams['axes.facecolor'] = ‘grey’ #调节背景颜色\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df2.loc[df2.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df2.loc[df2.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.20); ax.set_ylim(0, 45000) #调节纵坐标\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df2.loc[df2.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "    \n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
